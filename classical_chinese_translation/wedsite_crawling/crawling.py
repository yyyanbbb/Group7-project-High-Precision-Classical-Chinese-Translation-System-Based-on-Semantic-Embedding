# -*- coding: utf-8 -*-
import os, re, time, json, argparse, random
import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

BASE = "https://www.gushiwen.cn/shiwens/default.aspx"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/118.0.0.0 Safari/537.36",
    "Referer": "https://www.gushiwen.cn/",
    "Accept-Language": "zh-CN,zh;q=0.9",
}

# Ensure all data is saved to è¯—æ–‡æ•°æ® directory (absolute path)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.normpath(os.path.join(SCRIPT_DIR, "è¯—æ–‡æ•°æ®"))
os.makedirs(ROOT_DIR, exist_ok=True)

def get_save_directory() -> str:
    """Return the absolute path where all poem data will be saved."""
    return ROOT_DIR

# ä¼šè¯ + åŸºç¡€é‡è¯•
session = requests.Session()
retries = Retry(total=5, backoff_factor=2,
    status_forcelist=[429, 500, 502, 503, 504],
                allowed_methods=["GET", "HEAD"])
adapter = HTTPAdapter(max_retries=retries)
session.mount("http://", adapter)
session.mount("https://", adapter)

def safe_name(s: str) -> str:
    return "".join(c for c in s.strip() if c not in r'\/:*?"<>|')

def get_with_retry(url: str, max_retries: int = 5, base_sleep: float = 3.0) -> requests.Response:
    for i in range(1, max_retries+1):
        try:
            r = session.get(url, headers=HEADERS, timeout=30)
            r.raise_for_status()
            if len(r.content) < 512:
                raise Exception("å“åº”è¿‡çŸ­")
            return r
        except Exception as e:
            wait = base_sleep + random.random()*2
            print(f"âš ï¸ åˆ—è¡¨é¡µç¬¬ {i}/{max_retries} æ¬¡å¤±è´¥ï¼š{e}ï¼Œç­‰å¾… {wait:.1f}s")
            time.sleep(wait)
    raise Exception(f"å¤šæ¬¡é‡è¯•å¤±è´¥ï¼š{url}")

def fetch_ajax_translation(fid: str) -> str:
    for url in [f"https://www.gushiwen.cn/nocdn/ajaxfanyi.aspx?id={fid}",
                f"https://so.gushiwen.cn/nocdn/ajaxfanyi.aspx?id={fid}"]:
        try:
            r = session.get(url, headers=HEADERS, timeout=30)
            if r.status_code == 200 and r.text.strip():
                return BeautifulSoup(r.text, "lxml").get_text("\n", strip=True)
        except: pass
    return ""

def parse_inline_translation(soup: BeautifulSoup) -> str:
    for block in soup.select("div.contyishang"):
        text = block.get_text("\n", strip=True)
        if text and len(text) > 10:
            return text
    return ""

def fetch_detail(detail_url: str):
    r = session.get(detail_url, headers=HEADERS, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "lxml")

    title = soup.select_one("h1").get_text(strip=True)
    author = soup.select_one("p.source").get_text(strip=True) if soup.select_one("p.source") else ""
    content = soup.select_one("div.contson").get_text("\n", strip=True)

    translation = parse_inline_translation(soup)
    if not translation:
        btn = soup.find("a", onclick=re.compile(r"fanyiShow\(\d+\)"))
        if btn and btn.has_attr("onclick"):
            m = re.search(r"fanyiShow\((\d+)\)", btn["onclick"])
            if m:
                fid = m.group(1)
                translation = fetch_ajax_translation(fid)
    if not translation:
        translation = "æš‚æ— ç¿»è¯‘"

    return title, author, content, translation

def write_three_files(base_path: str, title: str, author: str, original: str, translation: str):
    poem_dir = os.path.join(base_path, safe_name(title))
    os.makedirs(poem_dir, exist_ok=True)

    with open(os.path.join(poem_dir, "åŸæ–‡.txt"), "w", encoding="utf-8") as f:
        f.write(f"æ ‡é¢˜ï¼š{title}\nä½œè€…ï¼š{author}\n\n{original}\n")

    with open(os.path.join(poem_dir, "è¯‘æ–‡.txt"), "w", encoding="utf-8") as f:
        f.write(f"æ ‡é¢˜ï¼š{title}\nä½œè€…ï¼š{author}\n\n{translation}\n")

    with open(os.path.join(poem_dir, "åŸæ–‡è¯‘æ–‡ç©¿æ’.txt"), "w", encoding="utf-8") as f:
        f.write(f"æ ‡é¢˜ï¼š{title}\nä½œè€…ï¼š{author}\n\n")
        ori_lines = [ln for ln in original.split("\n") if ln.strip()]
        trans_lines = [ln for ln in translation.split("\n") if ln.strip()] if translation != "æš‚æ— ç¿»è¯‘" else []
        max_len = max(len(ori_lines), len(trans_lines))
        for i in range(max_len):
            if i < len(ori_lines):
                f.write(ori_lines[i] + "\n")
            if i < len(trans_lines):
                f.write("è¯‘æ–‡ï¼š" + trans_lines[i] + "\n")

def state_file(category: str) -> str:
    return os.path.join(SCRIPT_DIR, f"crawl_state_{category}.json")

def failed_file(category: str) -> str:
    return os.path.join(SCRIPT_DIR, f"failed_links_{category}.json")

def load_state(category: str) -> dict:
    f = state_file(category)
    if os.path.exists(f):
        with open(f, "r", encoding="utf-8") as fh:
            return json.load(fh)
    return {"page": 1}

def save_state(category: str, state: dict):
    with open(state_file(category), "w", encoding="utf-8") as fh:
        json.dump(state, fh, ensure_ascii=False)

def load_failed(category: str) -> list:
    f = failed_file(category)
    if os.path.exists(f):
        with open(f, "r", encoding="utf-8") as fh:
            return json.load(fh)
    return []

def save_failed(category: str, failed_links: list):
    with open(failed_file(category), "w", encoding="utf-8") as fh:
        json.dump(failed_links, fh, ensure_ascii=False)

def crawl_pages(category: str, start_page: int = 1):
    state = load_state(category)
    page = max(start_page, state.get("page", 1))
    failed_links = load_failed(category)

    while True:
        list_url = f"{BASE}?xstr={category}&page={page}"
        try:
            r = get_with_retry(list_url)
        except Exception as e:
            print(f"âŒ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥ {list_url} - {e}")
            break

        soup = BeautifulSoup(r.text, "lxml")
        links = [a["href"] for a in soup.select("div.sons a[href*='shiwenv_']")]
        if not links:
            print("æ²¡æœ‰æ›´å¤šè¯—æ–‡ï¼Œçˆ¬å–ç»“æŸã€‚")
            break

        for href in links:
            detail_url = href if href.startswith("http") else "https://www.gushiwen.cn" + href
            try:
                title, author, original, translation = fetch_detail(detail_url)
                write_three_files(ROOT_DIR, title, author, original, translation)
                print(f"âœ… {title}")
            except Exception as e:
                print(f"âŒ {detail_url} - {e}")
                failed_links.append(detail_url)
            time.sleep(2)

        save_state(category, {"page": page})
        save_failed(category, failed_links)
        print(f"--- ç¬¬ {page} é¡µå®Œæˆï¼Œå·²ä¿å­˜æ–­ç‚¹å’Œå¤±è´¥é“¾æ¥ ---")

        next_btn = soup.find("a", string="ä¸‹ä¸€é¡µ")
        if next_btn:
            page += 1
            time.sleep(3)
        else:
            print("ğŸ“– å·²åˆ°æœ€åä¸€é¡µï¼Œçˆ¬å–å®Œæˆã€‚")
            break

    if failed_links:
        print(f"ğŸ”„ å¼€å§‹é‡è¯• {len(failed_links)} ä¸ªå¤±è´¥é“¾æ¥...")
        still_failed = []
    for url in failed_links:
        try:
            title, author, original, translation = fetch_detail(url)
            write_three_files(ROOT_DIR, title, author, original, translation)
            print(f"âœ… é‡è¯•æˆåŠŸï¼š{title}")
        except Exception as e:
            print(f"âŒ é‡è¯•å¤±è´¥ {url} - {e}")
            still_failed.append(url)
        time.sleep(2)
        save_failed(category, still_failed)

def main():
    # Show save directory at startup
    print("=" * 60)
    print(f"ğŸ“ æ•°æ®ä¿å­˜ç›®å½•: {ROOT_DIR}")
    print("=" * 60)
    
    print("\nè¯·é€‰æ‹©è¦çˆ¬å–çš„ç±»å‹ï¼š")
    print("1. è¯—")
    print("2. è¯")
    print("3. æ›²")
    print("4. æ–‡è¨€æ–‡")
    print("5. å…¨éƒ¨ï¼ˆä¾æ¬¡çˆ¬å–æ‰€æœ‰ç±»å‹ï¼‰")
    print("0. é€€å‡º")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-5): ").strip()
    
    category_map = {
        "1": "è¯—",
        "2": "è¯", 
        "3": "æ›²",
        "4": "æ–‡è¨€æ–‡"
    }
    
    if choice == "0":
        print("å·²é€€å‡ºã€‚")
        return
    elif choice == "5":
        # Crawl all categories
        for cat_name in ["è¯—", "è¯", "æ›²", "æ–‡è¨€æ–‡"]:
            print(f"\n{'='*60}")
            print(f"ğŸ”„ å¼€å§‹çˆ¬å–ï¼š{cat_name}")
            print(f"ğŸ“ ä¿å­˜åˆ°: {ROOT_DIR}")
            print(f"{'='*60}")
            crawl_pages(cat_name)
    elif choice in category_map:
        cat_name = category_map[choice]
        print(f"\n{'='*60}")
        print(f"ğŸ”„ å¼€å§‹çˆ¬å–ï¼š{cat_name}")
        print(f"ğŸ“ ä¿å­˜åˆ°: {ROOT_DIR}")
        print(f"{'='*60}")
        crawl_pages(cat_name)
    else:
        print("æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¿è¡Œç¨‹åºã€‚")
        return
    
    # Summary
    if os.path.exists(ROOT_DIR):
        poem_count = len([d for d in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, d))])
        print(f"\n{'='*60}")
        print(f"âœ… çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“ æ•°æ®ç›®å½•: {ROOT_DIR}")
        print(f"ğŸ“š å…±æœ‰ {poem_count} ç¯‡è¯—æ–‡")
        print(f"{'='*60}")


if __name__ == "__main__":
    main()
