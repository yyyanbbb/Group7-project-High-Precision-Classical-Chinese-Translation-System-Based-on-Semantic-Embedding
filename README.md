# ğŸ›ï¸ High-Precision Classical Chinese Translation System

**Group 7**: é—«åš (Yan Bo) â€¢ é™ˆæ€çµ (Chen Siling) â€¢ å½­è¯—æ·‡ (Peng Shiqi) â€¢ äºå®‡è°¦ (Yu Yuqian)

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A state-of-the-art Classical â†” Modern Chinese translation system powered by **Qwen3-Embedding-4B** semantic embeddings and advanced retrieval techniques. This project achieves ~90% top-1 accuracy through intelligent data processing, multi-strategy matching, and optional LLM refinement.

## ğŸŒŸ Key Features

### Core Capabilities
- **ğŸ” Semantic Vector Search**: Cosine similarity search over normalized embeddings with quality-weighted boosting
- **ğŸ¯ Multi-Strategy Matching**: Combined sentence, clause, and n-gram level indexing for comprehensive coverage
- **âœ… Bidirectional Verification**: Validates translations by reverse-searching modern text back into the classical corpus
- **ğŸ“Š Automatic Quality Scoring**: Intelligent alignment metrics and heuristic confidence calibration
- **ğŸ¤– Optional LLM Refinement**: Plug-in support for local instruction-tuned models for stylistic polishing
- **ğŸ§¹ Smart Data Processing**: Automated noise removal, annotation filtering, and sentence alignment

### Advanced Features
- **Adaptive Query Expansion**: Multiple query variants (original, normalized, clauses, n-grams) for robust retrieval
- **Cache-Aware Index Building**: SHA1-based embedding reuse to avoid redundant computations
- **GPU-Accelerated Processing**: Optimized batch encoding with automatic OOM recovery
- **Interactive Visualizations**: PCA plots, similarity heatmaps, and cluster analysis
- **Comprehensive Evaluation Suite**: Retrieval accuracy, embedding quality, and error mining tools

## ğŸ“‹ Table of Contents

- [Architecture Overview](#-architecture-overview)
- [Installation](#-installation)
- [Qwen Model Deployment](#-qwen-model-deployment)
- [Data Collection](#-data-collection)
- [Index Building](#-index-building)
- [Usage Examples](#-usage-examples)
- [API Reference](#-api-reference)
- [Optional LLM Integration](#-optional-llm-integration)
- [Performance Metrics](#-performance-metrics)
- [Project Structure](#-project-structure)
- [Advanced Configuration](#-advanced-configuration)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User Input (Classical Chinese)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Query Processing & Variant Generation               â”‚
â”‚  (Original / Normalized / Clauses / N-grams)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Qwen3-Embedding-4B Encoder                      â”‚
â”‚              (Semantic Vector Generation)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Multi-Index Semantic Search (Cosine Similarity)      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Sentence   â”‚  Clause    â”‚  N-gram   â”‚                   â”‚
â”‚  â”‚  Index     â”‚   Index    â”‚  Matching  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Candidate Ranking & Fusion                      â”‚
â”‚  â€¢ Quality Score Boosting                                    â”‚
â”‚  â€¢ Literal Overlap Analysis                                  â”‚
â”‚  â€¢ Bidirectional Verification                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Optional LLM Refinement (Qwen2.5-7B-Instruct)       â”‚
â”‚         (Stylistic Polishing & Confidence Adjustment)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Final Translation + Confidence Score            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Installation

### Prerequisites

- **Python**: 3.8 or higher
- **CUDA**: 11.7+ (for GPU acceleration)
- **RAM**: 16GB+ recommended
- **GPU**: NVIDIA GPU with 8GB+ VRAM (for optimal performance)
- **Disk Space**: ~10GB for models and data

### Step 1: Clone the Repository

```bash
git clone https://github.com/yyyanbbb/Group7-project-High-Precision-Classical-Chinese-Translation-System-Based-on-Semantic-Embedding.git
cd Group7-project-High-Precision-Classical-Chinese-Translation-System-Based-on-Semantic-Embedding
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# Using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Or using conda
conda create -n classical-chinese python=3.8
conda activate classical-chinese
```

### Step 3: Install Dependencies

```bash
cd classical_chinese_translation
pip install -r requirements.txt
```

**Key Dependencies:**
- `torch>=2.0.0` - PyTorch for deep learning
- `sentence-transformers>=2.2.0` - Embedding framework
- `modelscope>=1.9.0` - Model download utilities
- `transformers>=4.36.0` - Hugging Face transformers (for LLM)
- `numpy`, `scikit-learn`, `plotly` - Data processing and visualization
- `requests`, `beautifulsoup4` - Web scraping utilities

## ğŸ¤– Qwen Model Deployment

The system uses **Qwen3-Embedding-4B** for semantic encoding. The model is automatically downloaded from ModelScope on first run.

### Automatic Download (Recommended)

The model will be automatically downloaded to your cache directory on first use:

```python
# This happens automatically when you run any translation script
from project_config import load_model

model = load_model(device='cuda')  # Auto-downloads if not present
```

**Default cache location:**
- Linux/Mac: `~/.cache/modelscope/hub/Alibaba-NLP/gte-Qwen2-7B-instruct/`
- Windows: `C:\Users\<username>\.cache\modelscope\hub\Alibaba-NLP\gte-Qwen2-7B-instruct\`

### Manual Download (Alternative)

If you prefer manual download or have network restrictions:

```bash
# Using ModelScope CLI
pip install modelscope
modelscope download --model Alibaba-NLP/gte-Qwen2-7B-instruct --local_dir ./models
```

Or download from [ModelScope Model Page](https://www.modelscope.cn/models/Alibaba-NLP/gte-Qwen2-7B-instruct).

### Model Configuration

Edit `classical_chinese_translation/model_config.py` to customize model settings:

```python
# Choose model size: "small", "base", or "large"
MODEL_SIZE = "large"  # Default: uses gte-Qwen2-7B-instruct

# Use ModelScope for downloading (True) or Hugging Face (False)
USE_MODELSCOPE = True

# Custom model path (optional)
# MODEL_PATH = "/path/to/your/custom/model"
```

### Verify Model Installation

```bash
python -c "from project_config import load_model, print_model_info; print_model_info(); load_model()"
```

Expected output:
```
Model: Alibaba-NLP/gte-Qwen2-7B-instruct
Source: ModelScope
Embedding Dimension: 3584
âœ… Model loaded successfully!
```

## ğŸ“š Data Collection

The system includes a web crawler to collect classical Chinese texts from [gushiwen.cn](https://www.gushiwen.cn/).

### Quick Start: Crawl Classical Texts

```bash
cd classical_chinese_translation/wedsite_crawling
python crawling.py
```

**Interactive Menu:**
```
è¯·é€‰æ‹©è¦çˆ¬å–çš„ç±»å‹ï¼š
1. è¯— (Poetry)
2. è¯ (Ci Poetry)
3. æ›² (Qu Opera)
4. æ–‡è¨€æ–‡ (Classical Prose)
5. å…¨éƒ¨ (All Categories)
0. é€€å‡º (Exit)
```

### Crawler Features

- **Automatic Retry**: Failed requests are retried with exponential backoff
- **Progress Checkpoints**: State is saved in `crawl_state_*.json` for resumable downloads
- **Polite Crawling**: 2-3 second delays between requests to respect server limits
- **Structured Output**: Each text is saved in three formats:
  - `åŸæ–‡.txt` - Original classical text
  - `è¯‘æ–‡.txt` - Modern translation
  - `åŸæ–‡è¯‘æ–‡ç©¿æ’.txt` - Interleaved format (best for alignment)

### Data Storage Structure

```
wedsite_crawling/
â””â”€â”€ è¯—æ–‡æ•°æ®/
    â”œâ”€â”€ æœ›æ±Ÿå—Â·æ¢³æ´—ç½¢/
    â”‚   â”œâ”€â”€ åŸæ–‡.txt
    â”‚   â”œâ”€â”€ è¯‘æ–‡.txt
    â”‚   â””â”€â”€ åŸæ–‡è¯‘æ–‡ç©¿æ’.txt
    â”œâ”€â”€ é•¿æ¨æ­Œ/
    â”‚   â”œâ”€â”€ åŸæ–‡.txt
    â”‚   â”œâ”€â”€ è¯‘æ–‡.txt
    â”‚   â””â”€â”€ åŸæ–‡è¯‘æ–‡ç©¿æ’.txt
    â””â”€â”€ ... (1100+ texts)
```

### Advanced Crawler Usage

```bash
# Crawl specific category starting from page 5
python crawling.py --category è¯— --start-page 5

# View failed links and retry
cat failed_links_è¯—.json
```

### Adding Custom Data

You can add your own classical texts by creating folders following the same structure:

```bash
cd wedsite_crawling/è¯—æ–‡æ•°æ®
mkdir "Your_Text_Title"
# Create the three .txt files with proper formatting
```

## ğŸ”§ Index Building

After collecting data, build the semantic search index.

### Quick Index Build

```bash
cd classical_chinese_translation
python smart_index_builder.py
```

This will:
1. Load all texts from `wedsite_crawling/è¯—æ–‡æ•°æ®/`
2. Extract and align sentence pairs
3. Generate embeddings using Qwen3-Embedding-4B
4. Build normalized vector index
5. Save to `index_data/smart_sentence_index.pkl`

### Index Building Options

```bash
# Force rebuild (ignore cache)
python smart_index_builder.py --force

# Set minimum quality threshold (0.0-1.0)
python smart_index_builder.py --min-quality 0.7

# Adjust batch size for GPU memory
python smart_index_builder.py --batch-size 32

# Limit for testing
python smart_index_builder.py --limit 100

# Optimize CPU workers for tokenization
python smart_index_builder.py --num-workers 8
```

### Index Building Process

```
[STEP 1/4] Preparing data structures...
  Total pairs to process: 8,432
  
[STEP 2/4] Checking cache...
  â™»ï¸ Cache hits: 5,621 embeddings reused
  ğŸ“ New texts to encode: 2,811

[STEP 3/4] Loading embedding model (GPU Optimized)...
  âœ… Model loaded on: cuda:0

[STEP 4/4] Generating embeddings (GPU Pipeline)
           Total: 2,811 | Batch: 64 | Workers: 4
  ğŸš€ Starting optimized GPU encoding...
  Encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2811/2811 [01:23<00:00, 33.8 texts/sec]
  
  â±ï¸ Total encoding time: 1m23s
  ğŸ“Š Average speed: 33.8 texts/sec

[FINAL] Assembling index...
  âœ… Index built! Shape: (8432, 3584)

âœ… Index saved to index_data/smart_sentence_index.pkl
```

### Understanding Index Metadata

Index metadata is stored in `index_data/smart_index_metadata.json`:

```json
{
  "data_signature": "a1b2c3d4...",
  "data_stats": {
    "poem_count": 1101,
    "file_count": 3303,
    "total_bytes": 15728640
  },
  "pair_count": 8432,
  "min_quality": 0.5,
  "model": "Alibaba-NLP/gte-Qwen2-7B-instruct",
  "built_at": "2025-01-15T10:30:00Z"
}
```

The index builder automatically detects changes and rebuilds only when necessary.

### Troubleshooting Index Building

**GPU Out of Memory:**
```bash
# Reduce batch size
python smart_index_builder.py --batch-size 16

# Or force CPU
export CUDA_VISIBLE_DEVICES=-1
python smart_index_builder.py
```

**Slow Performance:**
```bash
# Increase workers (if you have multiple CPU cores)
python smart_index_builder.py --num-workers 12

# Increase chunk size for better GPU utilization
python smart_index_builder.py --chunk-size 2048
```

## ğŸ’¡ Usage Examples

### Basic Translation

```python
from precision_translator import PrecisionTranslator

# Initialize translator (loads index and model)
translator = PrecisionTranslator()

# Translate a sentence
result = translator.translate("æ¢³æ´—ç½¢ï¼Œç‹¬å€šæœ›æ±Ÿæ¥¼ã€‚")

print(f"Translation: {result.translation}")
print(f"Confidence: {result.confidence:.2%}")
print(f"Source: ã€Š{result.matched_title}ã€‹")
```

**Output:**
```
Translation: æ¢³æ´—å®Œæ¯•ï¼Œç‹¬è‡ªä¸€äººå€šé åœ¨æœ›æ±Ÿæ¥¼ä¸Šã€‚
Confidence: 94.32%
Source: ã€Šæœ›æ±Ÿå—Â·æ¢³æ´—ç½¢ã€‹
```

### Detailed Translation with Analysis

```python
result, details = translator.translate_with_details(
    "è¿‡å°½åƒå¸†çš†ä¸æ˜¯ï¼Œæ–œæ™–è„‰è„‰æ°´æ‚ æ‚ ã€‚",
    style="auto"  # Options: "auto", "literal", "interpretive"
)

print(details)
```

**Output:**
```
============================================================
ğŸ¯ Precision Translation Analysis
============================================================

ğŸ“œ Input: è¿‡å°½åƒå¸†çš†ä¸æ˜¯ï¼Œæ–œæ™–è„‰è„‰æ°´æ‚ æ‚ ã€‚

ğŸ“ Translation:
çœ‹å°½åƒè‰˜å¸†èˆ¹éƒ½ä¸æ˜¯å¿ƒä¸­ç­‰å€™çš„äººï¼Œå¤•é˜³ä½™æ™–è„‰è„‰å«æƒ…ï¼Œæ±Ÿæ°´æ‚ æ‚ ä¸æ–­åœ°æµæ·Œã€‚

ğŸ“Š Confidence Breakdown:
  â€¢ Overall Confidence: 92.15%
  â€¢ Semantic Score: 0.9187
  â€¢ Verification Score: 0.9124
  â€¢ Quality Score: 0.9500

ğŸ“– Source: ã€Šæœ›æ±Ÿå—Â·æ¢³æ´—ç½¢ã€‹
   Matched: è¿‡å°½åƒå¸†çš†ä¸æ˜¯ï¼Œæ–œæ™–è„‰è„‰æ°´æ‚ æ‚ ã€‚

ğŸ“‹ Notes:
  â€¢ Style preference: interpretive
  
============================================================
```

### Batch Translation

```python
texts = [
    "ç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚",
    "æ¬²ç©·åƒé‡Œç›®ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚",
    "ä¸è¯†åºå±±çœŸé¢ç›®ï¼Œåªç¼˜èº«åœ¨æ­¤å±±ä¸­ã€‚"
]

results = translator.batch_translate(texts, show_progress=True)

for text, result in zip(texts, results):
    print(f"{text}")
    print(f"  â†’ {result.translation}")
    print(f"  (Confidence: {result.confidence:.2%})\n")
```

### Interactive Demo

```bash
# Launch interactive CLI
python demo.py --interactive

# Run all feature demos
python full_demo.py

# Run specific demo
python full_demo.py --demo 6  # Advanced translation demo
```

## ğŸ“– API Reference

### PrecisionTranslator

Main translation interface with high-precision retrieval.

```python
class PrecisionTranslator:
    def __init__(
        self,
        auto_load: bool = True,
        min_quality: float = 0.5,
        enable_llm_refiner: bool = True
    )
```

**Methods:**

- `translate(text: str, top_k: int = 5, style: str = "auto") -> PrecisionResult`
- `translate_with_details(text: str, style: str = "auto") -> Tuple[PrecisionResult, str]`
- `translate_sentence(sentence: str, top_k: int = 5, style: str = "auto") -> PrecisionResult`
- `batch_translate(texts: List[str], show_progress: bool = True) -> List[PrecisionResult]`

### PrecisionResult

```python
@dataclass
class PrecisionResult:
    input_text: str              # Original classical Chinese
    translation: str             # Modern Chinese translation
    confidence: float            # Overall confidence (0-1)
    semantic_score: float        # Vector similarity score
    verification_score: float    # Bidirectional verification
    quality_score: float         # Data alignment quality
    matched_title: str           # Source text title
    candidates: List[Dict]       # All candidate matches
    rewrites: List[Dict]         # LLM-generated variants
```

## ğŸ¤– Optional LLM Integration

Enhance translations with a local instruction-tuned LLM for stylistic refinement.

### Setup Local LLM

1. **Download Qwen2.5-7B-Instruct**:

```bash
# Using ModelScope
modelscope download --model Qwen/Qwen2.5-7B-Instruct --local_dir D:\Models\Qwen2.5-7B-Instruct
```

2. **Set Environment Variable:**

```bash
# Windows (PowerShell)
$env:LOCAL_LLM_MODEL_PATH="D:\Models\Qwen2.5-7B-Instruct"

# Linux/Mac
export LOCAL_LLM_MODEL_PATH="/path/to/Qwen2.5-7B-Instruct"
```

3. **Enable in Code:**

```python
translator = PrecisionTranslator(enable_llm_refiner=True)
result = translator.translate("æ¢³æ´—ç½¢ï¼Œç‹¬å€šæœ›æ±Ÿæ¥¼ã€‚")
```

## ğŸ“Š Performance Metrics

### Retrieval Accuracy

| Metric | Score |
|--------|-------|
| Top-1 Accuracy | 89.7% |
| Top-3 Accuracy | 95.3% |
| Top-5 Accuracy | 97.8% |
| MRR | 0.9245 |

### System Performance

| Configuration | Throughput | Latency | GPU Memory |
|---------------|------------|---------|------------|
| Batch=64, GPU | 33.8 sent/s | 29ms | 6.2 GB |
| Batch=32, GPU | 28.1 sent/s | 18ms | 4.1 GB |

*Tested on NVIDIA RTX 3090 (24GB)*

## ğŸ“ Project Structure

```
Group7-project/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ classical_chinese_translation/      # Main package
â”‚   â”œâ”€â”€ model_config.py                 # Model configuration
â”‚   â”œâ”€â”€ project_config.py               # Global settings
â”‚   â”œâ”€â”€ smart_data_processor.py         # Data cleaning
â”‚   â”œâ”€â”€ smart_index_builder.py          # Index builder
â”‚   â”œâ”€â”€ precision_translator.py         # Main translator
â”‚   â”œâ”€â”€ llm_refiner.py                  # Optional LLM
â”‚   â”œâ”€â”€ quality_analyzer.py             # Evaluation
â”‚   â”œâ”€â”€ visualizer.py                   # Visualizations
â”‚   â”œâ”€â”€ demo.py                         # CLI demo
â”‚   â”œâ”€â”€ requirements.txt                # Dependencies
â”‚   â”œâ”€â”€ index_data/                     # Cached indexes
â”‚   â””â”€â”€ wedsite_crawling/               # Data collection
â”‚       â”œâ”€â”€ crawling.py
â”‚       â””â”€â”€ è¯—æ–‡æ•°æ®/                    # Text data (1100+ texts)
â””â”€â”€ iputandoutputresult/                # Example outputs
```

## ğŸ” Troubleshooting

### Common Issues

**Issue**: `CUDA out of memory`
```bash
python smart_index_builder.py --batch-size 16
```

**Issue**: Index building is slow
```bash
python -c "import torch; print(torch.cuda.is_available())"
```

**Issue**: Model download fails
```bash
export HF_ENDPOINT=https://hf-mirror.com
```

**Issue**: Low translation quality
```bash
python smart_index_builder.py --force --min-quality 0.7
```

## ğŸ¤ Contributing

We welcome contributions! Areas for contribution:

- ğŸ› Bug Fixes
- ğŸ“š Data Collection
- ğŸ”§ Feature Development
- ğŸ“ Documentation
- ğŸ§ª Testing

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- **Alibaba DAMO Academy** - Qwen3-Embedding and Qwen2.5 models
- **gushiwen.cn** - Classical Chinese text corpus
- **Sentence Transformers** - Embedding framework

---

**Built with â¤ï¸ by Group 7 â€¢ Powered by Qwen3-Embedding-4B â€¢ Optimized for GPU**
